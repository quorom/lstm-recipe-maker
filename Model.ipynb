{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from data_utils import Dictionary, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 128\n",
    "hidden_size = 2048\n",
    "num_layers = 2\n",
    "num_epochs = 500\n",
    "num_samples = 1000     # number of words to be sampled\n",
    "#batch_size = 20\n",
    "batch_size = 64\n",
    "seq_length = 30\n",
    "learning_rate = 0.1\n",
    "tanh = True\n",
    "Adam = False\n",
    "dropout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Adam:\n",
    "    batch_size = 20\n",
    "else:\n",
    "    batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actFunc = \"tanh\" if tanh else \"ReLU\"\n",
    "opt = \"Adam\" if Adam else \"SGD\"\n",
    "d_out = \"Yes\" if dropout else \"No\"\n",
    "title = \"LSTM-L-\"+str(num_layers) + \"-H-\" + str(hidden_size) + \"-Dropout-\" + d_out + \"-\" + actFunc + \"-\" + opt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM-L-2-H-2048-Dropout-No-tanh-SGD'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Penn Treebank\" dataset\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('data/cookbook.txt', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = None\n",
    "        if dropout:\n",
    "            self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout = 0.5, batch_first=True)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = None\n",
    "if Adam:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "CPU times: user 200 µs, sys: 58 µs, total: 258 µs\n",
      "Wall time: 185 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step[0/25], Loss: 8.3879, Perplexity: 4393.60\n",
      "Epoch [2/500], Step[0/25], Loss: 8.3109, Perplexity: 4067.92\n",
      "Epoch [3/500], Step[0/25], Loss: 8.2172, Perplexity: 3703.97\n",
      "Epoch [4/500], Step[0/25], Loss: 8.0573, Perplexity: 3156.77\n",
      "Epoch [5/500], Step[0/25], Loss: 7.4876, Perplexity: 1785.78\n",
      "Epoch [6/500], Step[0/25], Loss: 6.9374, Perplexity: 1030.04\n",
      "Epoch [7/500], Step[0/25], Loss: 6.6074, Perplexity: 740.53\n",
      "Epoch [8/500], Step[0/25], Loss: 6.4225, Perplexity: 615.57\n",
      "Epoch [9/500], Step[0/25], Loss: 6.3477, Perplexity: 571.20\n",
      "Epoch [10/500], Step[0/25], Loss: 6.2826, Perplexity: 535.20\n",
      "Epoch [11/500], Step[0/25], Loss: 6.2435, Perplexity: 514.64\n",
      "Epoch [12/500], Step[0/25], Loss: 6.2112, Perplexity: 498.30\n",
      "Epoch [13/500], Step[0/25], Loss: 6.1821, Perplexity: 484.03\n",
      "Epoch [14/500], Step[0/25], Loss: 6.1535, Perplexity: 470.35\n",
      "Epoch [15/500], Step[0/25], Loss: 6.1223, Perplexity: 455.92\n",
      "Epoch [16/500], Step[0/25], Loss: 6.1020, Perplexity: 446.76\n",
      "Epoch [17/500], Step[0/25], Loss: 6.0606, Perplexity: 428.61\n",
      "Epoch [18/500], Step[0/25], Loss: 6.0452, Perplexity: 422.07\n",
      "Epoch [19/500], Step[0/25], Loss: 6.0154, Perplexity: 409.67\n",
      "Epoch [20/500], Step[0/25], Loss: 6.0052, Perplexity: 405.52\n",
      "Epoch [21/500], Step[0/25], Loss: 5.9786, Perplexity: 394.89\n",
      "Epoch [22/500], Step[0/25], Loss: 5.9708, Perplexity: 391.84\n",
      "Epoch [23/500], Step[0/25], Loss: 5.9454, Perplexity: 381.99\n",
      "Epoch [24/500], Step[0/25], Loss: 5.9389, Perplexity: 379.54\n",
      "Epoch [25/500], Step[0/25], Loss: 5.9137, Perplexity: 370.06\n",
      "Epoch [26/500], Step[0/25], Loss: 5.9080, Perplexity: 367.97\n",
      "Epoch [27/500], Step[0/25], Loss: 5.8821, Perplexity: 358.58\n",
      "Epoch [28/500], Step[0/25], Loss: 5.8769, Perplexity: 356.70\n",
      "Epoch [29/500], Step[0/25], Loss: 5.8498, Perplexity: 347.16\n",
      "Epoch [30/500], Step[0/25], Loss: 5.8446, Perplexity: 345.36\n",
      "Epoch [31/500], Step[0/25], Loss: 5.8157, Perplexity: 335.53\n",
      "Epoch [32/500], Step[0/25], Loss: 5.8102, Perplexity: 333.67\n",
      "Epoch [33/500], Step[0/25], Loss: 5.7794, Perplexity: 323.56\n",
      "Epoch [34/500], Step[0/25], Loss: 5.7734, Perplexity: 321.63\n",
      "Epoch [35/500], Step[0/25], Loss: 5.7410, Perplexity: 311.39\n",
      "Epoch [36/500], Step[0/25], Loss: 5.7350, Perplexity: 309.52\n",
      "Epoch [37/500], Step[0/25], Loss: 5.7017, Perplexity: 299.37\n",
      "Epoch [38/500], Step[0/25], Loss: 5.6965, Perplexity: 297.81\n",
      "Epoch [39/500], Step[0/25], Loss: 5.6629, Perplexity: 287.99\n",
      "Epoch [40/500], Step[0/25], Loss: 5.6592, Perplexity: 286.91\n",
      "Epoch [41/500], Step[0/25], Loss: 5.6260, Perplexity: 277.54\n",
      "Epoch [42/500], Step[0/25], Loss: 5.6237, Perplexity: 276.92\n",
      "Epoch [43/500], Step[0/25], Loss: 5.5913, Perplexity: 268.09\n",
      "Epoch [44/500], Step[0/25], Loss: 5.5905, Perplexity: 267.87\n",
      "Epoch [45/500], Step[0/25], Loss: 5.5591, Perplexity: 259.59\n",
      "Epoch [46/500], Step[0/25], Loss: 5.5595, Perplexity: 259.69\n",
      "Epoch [47/500], Step[0/25], Loss: 5.5293, Perplexity: 251.96\n",
      "Epoch [48/500], Step[0/25], Loss: 5.5305, Perplexity: 252.27\n",
      "Epoch [49/500], Step[0/25], Loss: 5.5017, Perplexity: 245.11\n",
      "Epoch [50/500], Step[0/25], Loss: 5.5036, Perplexity: 245.57\n",
      "Epoch [51/500], Step[0/25], Loss: 5.4761, Perplexity: 238.91\n",
      "Epoch [52/500], Step[0/25], Loss: 5.4784, Perplexity: 239.45\n",
      "Epoch [53/500], Step[0/25], Loss: 5.4520, Perplexity: 233.24\n",
      "Epoch [54/500], Step[0/25], Loss: 5.4545, Perplexity: 233.81\n",
      "Epoch [55/500], Step[0/25], Loss: 5.4293, Perplexity: 227.99\n",
      "Epoch [56/500], Step[0/25], Loss: 5.4318, Perplexity: 228.55\n",
      "Epoch [57/500], Step[0/25], Loss: 5.4076, Perplexity: 223.09\n",
      "Epoch [58/500], Step[0/25], Loss: 5.4099, Perplexity: 223.61\n",
      "Epoch [59/500], Step[0/25], Loss: 5.3866, Perplexity: 218.46\n",
      "Epoch [60/500], Step[0/25], Loss: 5.3888, Perplexity: 218.94\n",
      "Epoch [61/500], Step[0/25], Loss: 5.3662, Perplexity: 214.04\n",
      "Epoch [62/500], Step[0/25], Loss: 5.3684, Perplexity: 214.52\n",
      "Epoch [63/500], Step[0/25], Loss: 5.3463, Perplexity: 209.82\n",
      "Epoch [64/500], Step[0/25], Loss: 5.3486, Perplexity: 210.32\n",
      "Epoch [65/500], Step[0/25], Loss: 5.3267, Perplexity: 205.75\n",
      "Epoch [66/500], Step[0/25], Loss: 5.3291, Perplexity: 206.24\n",
      "Epoch [67/500], Step[0/25], Loss: 5.3072, Perplexity: 201.78\n",
      "Epoch [68/500], Step[0/25], Loss: 5.3095, Perplexity: 202.25\n",
      "Epoch [69/500], Step[0/25], Loss: 5.2878, Perplexity: 197.92\n",
      "Epoch [70/500], Step[0/25], Loss: 5.2902, Perplexity: 198.38\n",
      "Epoch [71/500], Step[0/25], Loss: 5.2688, Perplexity: 194.19\n",
      "Epoch [72/500], Step[0/25], Loss: 5.2712, Perplexity: 194.65\n",
      "Epoch [73/500], Step[0/25], Loss: 5.2502, Perplexity: 190.60\n",
      "Epoch [74/500], Step[0/25], Loss: 5.2527, Perplexity: 191.09\n",
      "Epoch [75/500], Step[0/25], Loss: 5.2321, Perplexity: 187.18\n",
      "Epoch [76/500], Step[0/25], Loss: 5.2349, Perplexity: 187.71\n",
      "Epoch [77/500], Step[0/25], Loss: 5.2147, Perplexity: 183.95\n",
      "Epoch [78/500], Step[0/25], Loss: 5.2176, Perplexity: 184.50\n",
      "Epoch [79/500], Step[0/25], Loss: 5.1978, Perplexity: 180.88\n",
      "Epoch [80/500], Step[0/25], Loss: 5.2009, Perplexity: 181.44\n",
      "Epoch [81/500], Step[0/25], Loss: 5.1814, Perplexity: 177.93\n",
      "Epoch [82/500], Step[0/25], Loss: 5.1846, Perplexity: 178.50\n",
      "Epoch [83/500], Step[0/25], Loss: 5.1656, Perplexity: 175.15\n",
      "Epoch [84/500], Step[0/25], Loss: 5.1690, Perplexity: 175.73\n",
      "Epoch [85/500], Step[0/25], Loss: 5.1506, Perplexity: 172.53\n",
      "Epoch [86/500], Step[0/25], Loss: 5.1541, Perplexity: 173.14\n",
      "Epoch [87/500], Step[0/25], Loss: 5.1360, Perplexity: 170.03\n",
      "Epoch [88/500], Step[0/25], Loss: 5.1392, Perplexity: 170.59\n",
      "Epoch [89/500], Step[0/25], Loss: 5.1218, Perplexity: 167.64\n",
      "Epoch [90/500], Step[0/25], Loss: 5.1249, Perplexity: 168.16\n",
      "Epoch [91/500], Step[0/25], Loss: 5.1079, Perplexity: 165.33\n",
      "Epoch [92/500], Step[0/25], Loss: 5.1109, Perplexity: 165.82\n",
      "Epoch [93/500], Step[0/25], Loss: 5.0945, Perplexity: 163.12\n",
      "Epoch [94/500], Step[0/25], Loss: 5.0973, Perplexity: 163.58\n",
      "Epoch [95/500], Step[0/25], Loss: 5.0813, Perplexity: 160.98\n",
      "Epoch [96/500], Step[0/25], Loss: 5.0839, Perplexity: 161.41\n",
      "Epoch [97/500], Step[0/25], Loss: 5.0683, Perplexity: 158.91\n",
      "Epoch [98/500], Step[0/25], Loss: 5.0708, Perplexity: 159.31\n",
      "Epoch [99/500], Step[0/25], Loss: 5.0556, Perplexity: 156.89\n",
      "Epoch [100/500], Step[0/25], Loss: 5.0579, Perplexity: 157.26\n",
      "Epoch [101/500], Step[0/25], Loss: 5.0430, Perplexity: 154.93\n",
      "Epoch [102/500], Step[0/25], Loss: 5.0450, Perplexity: 155.25\n",
      "Epoch [103/500], Step[0/25], Loss: 5.0305, Perplexity: 153.01\n",
      "Epoch [104/500], Step[0/25], Loss: 5.0323, Perplexity: 153.29\n",
      "Epoch [105/500], Step[0/25], Loss: 5.0181, Perplexity: 151.13\n",
      "Epoch [106/500], Step[0/25], Loss: 5.0198, Perplexity: 151.38\n",
      "Epoch [107/500], Step[0/25], Loss: 5.0059, Perplexity: 149.30\n",
      "Epoch [108/500], Step[0/25], Loss: 5.0073, Perplexity: 149.50\n",
      "Epoch [109/500], Step[0/25], Loss: 4.9938, Perplexity: 147.50\n",
      "Epoch [110/500], Step[0/25], Loss: 4.9949, Perplexity: 147.66\n",
      "Epoch [111/500], Step[0/25], Loss: 4.9818, Perplexity: 145.73\n",
      "Epoch [112/500], Step[0/25], Loss: 4.9826, Perplexity: 145.86\n",
      "Epoch [113/500], Step[0/25], Loss: 4.9698, Perplexity: 144.01\n",
      "Epoch [114/500], Step[0/25], Loss: 4.9703, Perplexity: 144.07\n",
      "Epoch [115/500], Step[0/25], Loss: 4.9580, Perplexity: 142.31\n",
      "Epoch [116/500], Step[0/25], Loss: 4.9580, Perplexity: 142.32\n",
      "Epoch [117/500], Step[0/25], Loss: 4.9461, Perplexity: 140.63\n",
      "Epoch [118/500], Step[0/25], Loss: 4.9458, Perplexity: 140.59\n",
      "Epoch [119/500], Step[0/25], Loss: 4.9345, Perplexity: 139.00\n",
      "Epoch [120/500], Step[0/25], Loss: 4.9338, Perplexity: 138.90\n",
      "Epoch [121/500], Step[0/25], Loss: 4.9230, Perplexity: 137.41\n",
      "Epoch [122/500], Step[0/25], Loss: 4.9219, Perplexity: 137.26\n",
      "Epoch [123/500], Step[0/25], Loss: 4.9118, Perplexity: 135.88\n",
      "Epoch [124/500], Step[0/25], Loss: 4.9101, Perplexity: 135.65\n",
      "Epoch [125/500], Step[0/25], Loss: 4.9005, Perplexity: 134.36\n",
      "Epoch [126/500], Step[0/25], Loss: 4.8985, Perplexity: 134.09\n",
      "Epoch [127/500], Step[0/25], Loss: 4.8895, Perplexity: 132.88\n",
      "Epoch [128/500], Step[0/25], Loss: 4.8870, Perplexity: 132.55\n",
      "Epoch [129/500], Step[0/25], Loss: 4.8785, Perplexity: 131.43\n",
      "Epoch [130/500], Step[0/25], Loss: 4.8754, Perplexity: 131.03\n",
      "Epoch [131/500], Step[0/25], Loss: 4.8675, Perplexity: 130.00\n",
      "Epoch [132/500], Step[0/25], Loss: 4.8637, Perplexity: 129.51\n",
      "Epoch [133/500], Step[0/25], Loss: 4.8569, Perplexity: 128.63\n",
      "Epoch [134/500], Step[0/25], Loss: 4.8527, Perplexity: 128.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/500], Step[0/25], Loss: 4.8452, Perplexity: 127.13\n",
      "Epoch [136/500], Step[0/25], Loss: 4.8409, Perplexity: 126.59\n",
      "Epoch [137/500], Step[0/25], Loss: 4.8339, Perplexity: 125.70\n",
      "Epoch [138/500], Step[0/25], Loss: 4.8295, Perplexity: 125.15\n",
      "Epoch [139/500], Step[0/25], Loss: 4.8228, Perplexity: 124.31\n",
      "Epoch [140/500], Step[0/25], Loss: 4.8181, Perplexity: 123.73\n",
      "Epoch [141/500], Step[0/25], Loss: 4.8116, Perplexity: 122.93\n",
      "Epoch [142/500], Step[0/25], Loss: 4.8067, Perplexity: 122.33\n",
      "Epoch [143/500], Step[0/25], Loss: 4.8009, Perplexity: 121.63\n",
      "Epoch [144/500], Step[0/25], Loss: 4.7945, Perplexity: 120.84\n",
      "Epoch [145/500], Step[0/25], Loss: 4.7891, Perplexity: 120.19\n",
      "Epoch [146/500], Step[0/25], Loss: 4.7818, Perplexity: 119.32\n",
      "Epoch [147/500], Step[0/25], Loss: 4.7790, Perplexity: 118.99\n",
      "Epoch [148/500], Step[0/25], Loss: 4.7711, Perplexity: 118.05\n",
      "Epoch [149/500], Step[0/25], Loss: 4.7667, Perplexity: 117.53\n",
      "Epoch [150/500], Step[0/25], Loss: 4.7603, Perplexity: 116.78\n",
      "Epoch [151/500], Step[0/25], Loss: 4.7551, Perplexity: 116.17\n",
      "Epoch [152/500], Step[0/25], Loss: 4.7495, Perplexity: 115.52\n",
      "Epoch [153/500], Step[0/25], Loss: 4.7421, Perplexity: 114.67\n",
      "Epoch [154/500], Step[0/25], Loss: 4.7391, Perplexity: 114.33\n",
      "Epoch [155/500], Step[0/25], Loss: 4.7302, Perplexity: 113.32\n",
      "Epoch [156/500], Step[0/25], Loss: 4.7257, Perplexity: 112.81\n",
      "Epoch [157/500], Step[0/25], Loss: 4.7142, Perplexity: 111.52\n",
      "Epoch [158/500], Step[0/25], Loss: 4.7100, Perplexity: 111.05\n",
      "Epoch [159/500], Step[0/25], Loss: 4.7061, Perplexity: 110.62\n",
      "Epoch [160/500], Step[0/25], Loss: 4.7014, Perplexity: 110.10\n",
      "Epoch [161/500], Step[0/25], Loss: 4.6938, Perplexity: 109.27\n",
      "Epoch [162/500], Step[0/25], Loss: 4.6915, Perplexity: 109.02\n",
      "Epoch [163/500], Step[0/25], Loss: 4.6832, Perplexity: 108.12\n",
      "Epoch [164/500], Step[0/25], Loss: 4.6788, Perplexity: 107.64\n",
      "Epoch [165/500], Step[0/25], Loss: 4.6719, Perplexity: 106.90\n",
      "Epoch [166/500], Step[0/25], Loss: 4.6668, Perplexity: 106.35\n",
      "Epoch [167/500], Step[0/25], Loss: 4.6593, Perplexity: 105.57\n",
      "Epoch [168/500], Step[0/25], Loss: 4.6558, Perplexity: 105.20\n",
      "Epoch [169/500], Step[0/25], Loss: 4.6494, Perplexity: 104.52\n",
      "Epoch [170/500], Step[0/25], Loss: 4.6441, Perplexity: 103.97\n",
      "Epoch [171/500], Step[0/25], Loss: 4.6349, Perplexity: 103.01\n",
      "Epoch [172/500], Step[0/25], Loss: 4.6324, Perplexity: 102.76\n",
      "Epoch [173/500], Step[0/25], Loss: 4.6231, Perplexity: 101.81\n",
      "Epoch [174/500], Step[0/25], Loss: 4.6185, Perplexity: 101.34\n",
      "Epoch [175/500], Step[0/25], Loss: 4.6150, Perplexity: 100.99\n",
      "Epoch [176/500], Step[0/25], Loss: 4.6086, Perplexity: 100.35\n",
      "Epoch [177/500], Step[0/25], Loss: 4.6017, Perplexity: 99.65\n",
      "Epoch [178/500], Step[0/25], Loss: 4.5967, Perplexity: 99.15\n",
      "Epoch [179/500], Step[0/25], Loss: 4.5936, Perplexity: 98.84\n",
      "Epoch [180/500], Step[0/25], Loss: 4.5852, Perplexity: 98.02\n",
      "Epoch [181/500], Step[0/25], Loss: 4.5833, Perplexity: 97.83\n",
      "Epoch [182/500], Step[0/25], Loss: 4.5756, Perplexity: 97.09\n",
      "Epoch [183/500], Step[0/25], Loss: 4.5719, Perplexity: 96.73\n",
      "Epoch [184/500], Step[0/25], Loss: 4.5635, Perplexity: 95.91\n",
      "Epoch [185/500], Step[0/25], Loss: 4.5601, Perplexity: 95.59\n",
      "Epoch [186/500], Step[0/25], Loss: 4.5523, Perplexity: 94.85\n",
      "Epoch [187/500], Step[0/25], Loss: 4.5556, Perplexity: 95.16\n",
      "Epoch [188/500], Step[0/25], Loss: 4.5425, Perplexity: 93.92\n",
      "Epoch [189/500], Step[0/25], Loss: 4.5384, Perplexity: 93.54\n",
      "Epoch [190/500], Step[0/25], Loss: 4.5310, Perplexity: 92.85\n",
      "Epoch [191/500], Step[0/25], Loss: 4.5321, Perplexity: 92.96\n",
      "Epoch [192/500], Step[0/25], Loss: 4.5209, Perplexity: 91.92\n",
      "Epoch [193/500], Step[0/25], Loss: 4.5161, Perplexity: 91.48\n",
      "Epoch [194/500], Step[0/25], Loss: 4.5085, Perplexity: 90.79\n",
      "Epoch [195/500], Step[0/25], Loss: 4.5042, Perplexity: 90.39\n",
      "Epoch [196/500], Step[0/25], Loss: 4.4969, Perplexity: 89.74\n",
      "Epoch [197/500], Step[0/25], Loss: 4.4910, Perplexity: 89.21\n",
      "Epoch [198/500], Step[0/25], Loss: 4.4978, Perplexity: 89.82\n",
      "Epoch [199/500], Step[0/25], Loss: 4.4855, Perplexity: 88.73\n",
      "Epoch [200/500], Step[0/25], Loss: 4.4761, Perplexity: 87.90\n",
      "Epoch [201/500], Step[0/25], Loss: 4.4839, Perplexity: 88.58\n",
      "Epoch [202/500], Step[0/25], Loss: 4.4683, Perplexity: 87.21\n",
      "Epoch [203/500], Step[0/25], Loss: 4.4661, Perplexity: 87.02\n",
      "Epoch [204/500], Step[0/25], Loss: 4.4627, Perplexity: 86.72\n",
      "Epoch [205/500], Step[0/25], Loss: 4.4558, Perplexity: 86.13\n",
      "Epoch [206/500], Step[0/25], Loss: 4.4527, Perplexity: 85.86\n",
      "Epoch [207/500], Step[0/25], Loss: 4.4466, Perplexity: 85.34\n",
      "Epoch [208/500], Step[0/25], Loss: 4.4406, Perplexity: 84.82\n",
      "Epoch [209/500], Step[0/25], Loss: 4.4323, Perplexity: 84.12\n",
      "Epoch [210/500], Step[0/25], Loss: 4.4330, Perplexity: 84.18\n",
      "Epoch [211/500], Step[0/25], Loss: 4.4238, Perplexity: 83.41\n",
      "Epoch [212/500], Step[0/25], Loss: 4.4205, Perplexity: 83.13\n",
      "Epoch [213/500], Step[0/25], Loss: 4.4118, Perplexity: 82.42\n",
      "Epoch [214/500], Step[0/25], Loss: 4.4070, Perplexity: 82.02\n",
      "Epoch [215/500], Step[0/25], Loss: 4.4003, Perplexity: 81.47\n",
      "Epoch [216/500], Step[0/25], Loss: 4.4020, Perplexity: 81.61\n",
      "Epoch [217/500], Step[0/25], Loss: 4.3917, Perplexity: 80.78\n",
      "Epoch [218/500], Step[0/25], Loss: 4.3860, Perplexity: 80.32\n",
      "Epoch [219/500], Step[0/25], Loss: 4.3819, Perplexity: 79.99\n",
      "Epoch [220/500], Step[0/25], Loss: 4.3758, Perplexity: 79.50\n",
      "Epoch [221/500], Step[0/25], Loss: 4.3733, Perplexity: 79.31\n",
      "Epoch [222/500], Step[0/25], Loss: 4.3760, Perplexity: 79.52\n",
      "Epoch [223/500], Step[0/25], Loss: 4.3642, Perplexity: 78.59\n",
      "Epoch [224/500], Step[0/25], Loss: 4.3620, Perplexity: 78.41\n",
      "Epoch [225/500], Step[0/25], Loss: 4.3611, Perplexity: 78.35\n",
      "Epoch [226/500], Step[0/25], Loss: 4.3513, Perplexity: 77.58\n",
      "Epoch [227/500], Step[0/25], Loss: 4.3457, Perplexity: 77.15\n",
      "Epoch [228/500], Step[0/25], Loss: 4.3435, Perplexity: 76.98\n",
      "Epoch [229/500], Step[0/25], Loss: 4.3353, Perplexity: 76.35\n",
      "Epoch [230/500], Step[0/25], Loss: 4.3383, Perplexity: 76.58\n",
      "Epoch [231/500], Step[0/25], Loss: 4.3268, Perplexity: 75.70\n",
      "Epoch [232/500], Step[0/25], Loss: 4.3326, Perplexity: 76.14\n",
      "Epoch [233/500], Step[0/25], Loss: 4.3182, Perplexity: 75.06\n",
      "Epoch [234/500], Step[0/25], Loss: 4.3207, Perplexity: 75.24\n",
      "Epoch [235/500], Step[0/25], Loss: 4.3093, Perplexity: 74.39\n",
      "Epoch [236/500], Step[0/25], Loss: 4.3194, Perplexity: 75.15\n",
      "Epoch [237/500], Step[0/25], Loss: 4.3011, Perplexity: 73.78\n",
      "Epoch [238/500], Step[0/25], Loss: 4.2973, Perplexity: 73.50\n",
      "Epoch [239/500], Step[0/25], Loss: 4.3019, Perplexity: 73.84\n",
      "Epoch [240/500], Step[0/25], Loss: 4.2865, Perplexity: 72.71\n",
      "Epoch [241/500], Step[0/25], Loss: 4.2915, Perplexity: 73.08\n",
      "Epoch [242/500], Step[0/25], Loss: 4.2790, Perplexity: 72.17\n",
      "Epoch [243/500], Step[0/25], Loss: 4.2792, Perplexity: 72.18\n",
      "Epoch [244/500], Step[0/25], Loss: 4.2846, Perplexity: 72.58\n",
      "Epoch [245/500], Step[0/25], Loss: 4.2670, Perplexity: 71.31\n",
      "Epoch [246/500], Step[0/25], Loss: 4.2628, Perplexity: 71.01\n",
      "Epoch [247/500], Step[0/25], Loss: 4.2569, Perplexity: 70.59\n",
      "Epoch [248/500], Step[0/25], Loss: 4.2635, Perplexity: 71.06\n",
      "Epoch [249/500], Step[0/25], Loss: 4.2453, Perplexity: 69.78\n",
      "Epoch [250/500], Step[0/25], Loss: 4.2524, Perplexity: 70.27\n",
      "Epoch [251/500], Step[0/25], Loss: 4.2429, Perplexity: 69.61\n",
      "Epoch [252/500], Step[0/25], Loss: 4.2396, Perplexity: 69.38\n",
      "Epoch [253/500], Step[0/25], Loss: 4.2290, Perplexity: 68.65\n",
      "Epoch [254/500], Step[0/25], Loss: 4.2304, Perplexity: 68.74\n",
      "Epoch [255/500], Step[0/25], Loss: 4.2236, Perplexity: 68.28\n",
      "Epoch [256/500], Step[0/25], Loss: 4.2169, Perplexity: 67.82\n",
      "Epoch [257/500], Step[0/25], Loss: 4.2124, Perplexity: 67.52\n",
      "Epoch [258/500], Step[0/25], Loss: 4.2120, Perplexity: 67.49\n",
      "Epoch [259/500], Step[0/25], Loss: 4.2034, Perplexity: 66.91\n",
      "Epoch [260/500], Step[0/25], Loss: 4.2018, Perplexity: 66.80\n",
      "Epoch [261/500], Step[0/25], Loss: 4.1946, Perplexity: 66.33\n",
      "Epoch [262/500], Step[0/25], Loss: 4.1860, Perplexity: 65.76\n",
      "Epoch [263/500], Step[0/25], Loss: 4.1836, Perplexity: 65.60\n",
      "Epoch [264/500], Step[0/25], Loss: 4.1977, Perplexity: 66.53\n",
      "Epoch [265/500], Step[0/25], Loss: 4.1770, Perplexity: 65.17\n",
      "Epoch [266/500], Step[0/25], Loss: 4.1727, Perplexity: 64.89\n",
      "Epoch [267/500], Step[0/25], Loss: 4.1763, Perplexity: 65.12\n",
      "Epoch [268/500], Step[0/25], Loss: 4.1638, Perplexity: 64.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [269/500], Step[0/25], Loss: 4.1650, Perplexity: 64.39\n",
      "Epoch [270/500], Step[0/25], Loss: 4.1541, Perplexity: 63.70\n",
      "Epoch [271/500], Step[0/25], Loss: 4.1491, Perplexity: 63.38\n",
      "Epoch [272/500], Step[0/25], Loss: 4.1477, Perplexity: 63.29\n",
      "Epoch [273/500], Step[0/25], Loss: 4.1442, Perplexity: 63.07\n",
      "Epoch [274/500], Step[0/25], Loss: 4.1373, Perplexity: 62.63\n",
      "Epoch [275/500], Step[0/25], Loss: 4.1312, Perplexity: 62.25\n",
      "Epoch [276/500], Step[0/25], Loss: 4.1319, Perplexity: 62.30\n",
      "Epoch [277/500], Step[0/25], Loss: 4.1203, Perplexity: 61.58\n",
      "Epoch [278/500], Step[0/25], Loss: 4.1167, Perplexity: 61.36\n",
      "Epoch [279/500], Step[0/25], Loss: 4.1115, Perplexity: 61.04\n",
      "Epoch [280/500], Step[0/25], Loss: 4.1069, Perplexity: 60.76\n",
      "Epoch [281/500], Step[0/25], Loss: 4.1153, Perplexity: 61.27\n",
      "Epoch [282/500], Step[0/25], Loss: 4.1011, Perplexity: 60.41\n",
      "Epoch [283/500], Step[0/25], Loss: 4.0987, Perplexity: 60.26\n",
      "Epoch [284/500], Step[0/25], Loss: 4.1066, Perplexity: 60.74\n",
      "Epoch [285/500], Step[0/25], Loss: 4.0901, Perplexity: 59.75\n",
      "Epoch [286/500], Step[0/25], Loss: 4.0860, Perplexity: 59.50\n",
      "Epoch [287/500], Step[0/25], Loss: 4.0842, Perplexity: 59.40\n",
      "Epoch [288/500], Step[0/25], Loss: 4.0784, Perplexity: 59.05\n",
      "Epoch [289/500], Step[0/25], Loss: 4.0806, Perplexity: 59.18\n",
      "Epoch [290/500], Step[0/25], Loss: 4.0693, Perplexity: 58.52\n",
      "Epoch [291/500], Step[0/25], Loss: 4.0675, Perplexity: 58.41\n",
      "Epoch [292/500], Step[0/25], Loss: 4.0745, Perplexity: 58.82\n",
      "Epoch [293/500], Step[0/25], Loss: 4.0585, Perplexity: 57.89\n",
      "Epoch [294/500], Step[0/25], Loss: 4.0557, Perplexity: 57.72\n",
      "Epoch [295/500], Step[0/25], Loss: 4.0613, Perplexity: 58.05\n",
      "Epoch [296/500], Step[0/25], Loss: 4.0450, Perplexity: 57.11\n",
      "Epoch [297/500], Step[0/25], Loss: 4.0460, Perplexity: 57.17\n",
      "Epoch [298/500], Step[0/25], Loss: 4.0404, Perplexity: 56.85\n",
      "Epoch [299/500], Step[0/25], Loss: 4.0351, Perplexity: 56.55\n",
      "Epoch [300/500], Step[0/25], Loss: 4.0411, Perplexity: 56.89\n",
      "Epoch [301/500], Step[0/25], Loss: 4.0302, Perplexity: 56.27\n",
      "Epoch [302/500], Step[0/25], Loss: 4.0304, Perplexity: 56.28\n",
      "Epoch [303/500], Step[0/25], Loss: 4.0544, Perplexity: 57.65\n",
      "Epoch [304/500], Step[0/25], Loss: 4.0193, Perplexity: 55.66\n",
      "Epoch [305/500], Step[0/25], Loss: 4.0104, Perplexity: 55.17\n",
      "Epoch [306/500], Step[0/25], Loss: 4.0135, Perplexity: 55.34\n",
      "Epoch [307/500], Step[0/25], Loss: 4.0063, Perplexity: 54.94\n",
      "Epoch [308/500], Step[0/25], Loss: 4.0056, Perplexity: 54.91\n",
      "Epoch [309/500], Step[0/25], Loss: 4.0224, Perplexity: 55.83\n",
      "Epoch [310/500], Step[0/25], Loss: 3.9989, Perplexity: 54.54\n",
      "Epoch [311/500], Step[0/25], Loss: 3.9946, Perplexity: 54.30\n",
      "Epoch [312/500], Step[0/25], Loss: 3.9886, Perplexity: 53.98\n",
      "Epoch [313/500], Step[0/25], Loss: 3.9853, Perplexity: 53.80\n",
      "Epoch [314/500], Step[0/25], Loss: 3.9877, Perplexity: 53.93\n",
      "Epoch [315/500], Step[0/25], Loss: 3.9793, Perplexity: 53.48\n",
      "Epoch [316/500], Step[0/25], Loss: 3.9724, Perplexity: 53.11\n",
      "Epoch [317/500], Step[0/25], Loss: 3.9712, Perplexity: 53.05\n",
      "Epoch [318/500], Step[0/25], Loss: 4.0015, Perplexity: 54.68\n",
      "Epoch [319/500], Step[0/25], Loss: 3.9683, Perplexity: 52.89\n",
      "Epoch [320/500], Step[0/25], Loss: 3.9598, Perplexity: 52.45\n",
      "Epoch [321/500], Step[0/25], Loss: 3.9599, Perplexity: 52.45\n",
      "Epoch [322/500], Step[0/25], Loss: 3.9707, Perplexity: 53.02\n",
      "Epoch [323/500], Step[0/25], Loss: 3.9647, Perplexity: 52.71\n",
      "Epoch [324/500], Step[0/25], Loss: 3.9516, Perplexity: 52.02\n",
      "Epoch [325/500], Step[0/25], Loss: 3.9452, Perplexity: 51.69\n",
      "Epoch [326/500], Step[0/25], Loss: 3.9461, Perplexity: 51.73\n",
      "Epoch [327/500], Step[0/25], Loss: 3.9421, Perplexity: 51.53\n",
      "Epoch [328/500], Step[0/25], Loss: 3.9360, Perplexity: 51.22\n",
      "Epoch [329/500], Step[0/25], Loss: 3.9334, Perplexity: 51.08\n",
      "Epoch [330/500], Step[0/25], Loss: 3.9366, Perplexity: 51.24\n",
      "Epoch [331/500], Step[0/25], Loss: 3.9261, Perplexity: 50.71\n",
      "Epoch [332/500], Step[0/25], Loss: 3.9285, Perplexity: 50.83\n",
      "Epoch [333/500], Step[0/25], Loss: 3.9206, Perplexity: 50.43\n",
      "Epoch [334/500], Step[0/25], Loss: 3.9185, Perplexity: 50.33\n",
      "Epoch [335/500], Step[0/25], Loss: 3.9083, Perplexity: 49.82\n",
      "Epoch [336/500], Step[0/25], Loss: 3.9070, Perplexity: 49.75\n",
      "Epoch [337/500], Step[0/25], Loss: 3.9050, Perplexity: 49.65\n",
      "Epoch [338/500], Step[0/25], Loss: 3.9034, Perplexity: 49.57\n",
      "Epoch [339/500], Step[0/25], Loss: 3.9182, Perplexity: 50.31\n",
      "Epoch [340/500], Step[0/25], Loss: 3.8985, Perplexity: 49.33\n",
      "Epoch [341/500], Step[0/25], Loss: 3.8961, Perplexity: 49.21\n",
      "Epoch [342/500], Step[0/25], Loss: 3.8918, Perplexity: 49.00\n",
      "Epoch [343/500], Step[0/25], Loss: 3.8963, Perplexity: 49.22\n",
      "Epoch [344/500], Step[0/25], Loss: 3.8939, Perplexity: 49.10\n",
      "Epoch [345/500], Step[0/25], Loss: 3.8807, Perplexity: 48.46\n",
      "Epoch [346/500], Step[0/25], Loss: 3.8784, Perplexity: 48.35\n",
      "Epoch [347/500], Step[0/25], Loss: 3.9068, Perplexity: 49.74\n",
      "Epoch [348/500], Step[0/25], Loss: 3.8706, Perplexity: 47.97\n",
      "Epoch [349/500], Step[0/25], Loss: 3.8684, Perplexity: 47.87\n",
      "Epoch [350/500], Step[0/25], Loss: 3.8643, Perplexity: 47.67\n",
      "Epoch [351/500], Step[0/25], Loss: 3.8608, Perplexity: 47.50\n",
      "Epoch [352/500], Step[0/25], Loss: 3.8759, Perplexity: 48.22\n",
      "Epoch [353/500], Step[0/25], Loss: 3.8553, Perplexity: 47.24\n",
      "Epoch [354/500], Step[0/25], Loss: 3.8518, Perplexity: 47.08\n",
      "Epoch [355/500], Step[0/25], Loss: 3.8461, Perplexity: 46.81\n",
      "Epoch [356/500], Step[0/25], Loss: 3.8497, Perplexity: 46.98\n",
      "Epoch [357/500], Step[0/25], Loss: 3.8461, Perplexity: 46.81\n",
      "Epoch [358/500], Step[0/25], Loss: 3.8384, Perplexity: 46.45\n",
      "Epoch [359/500], Step[0/25], Loss: 3.8328, Perplexity: 46.19\n",
      "Epoch [360/500], Step[0/25], Loss: 3.8530, Perplexity: 47.13\n",
      "Epoch [361/500], Step[0/25], Loss: 3.8305, Perplexity: 46.09\n",
      "Epoch [362/500], Step[0/25], Loss: 3.8363, Perplexity: 46.35\n",
      "Epoch [363/500], Step[0/25], Loss: 3.8253, Perplexity: 45.85\n",
      "Epoch [364/500], Step[0/25], Loss: 3.8178, Perplexity: 45.51\n",
      "Epoch [365/500], Step[0/25], Loss: 3.8217, Perplexity: 45.68\n",
      "Epoch [366/500], Step[0/25], Loss: 3.8153, Perplexity: 45.39\n",
      "Epoch [367/500], Step[0/25], Loss: 3.8143, Perplexity: 45.34\n",
      "Epoch [368/500], Step[0/25], Loss: 3.8049, Perplexity: 44.92\n",
      "Epoch [369/500], Step[0/25], Loss: 3.8086, Perplexity: 45.09\n",
      "Epoch [370/500], Step[0/25], Loss: 3.8024, Perplexity: 44.81\n",
      "Epoch [371/500], Step[0/25], Loss: 3.8143, Perplexity: 45.35\n",
      "Epoch [372/500], Step[0/25], Loss: 3.7953, Perplexity: 44.49\n",
      "Epoch [373/500], Step[0/25], Loss: 3.7925, Perplexity: 44.37\n",
      "Epoch [374/500], Step[0/25], Loss: 3.7902, Perplexity: 44.27\n",
      "Epoch [375/500], Step[0/25], Loss: 3.7896, Perplexity: 44.24\n",
      "Epoch [376/500], Step[0/25], Loss: 3.7902, Perplexity: 44.27\n",
      "Epoch [377/500], Step[0/25], Loss: 3.7774, Perplexity: 43.70\n",
      "Epoch [378/500], Step[0/25], Loss: 3.7751, Perplexity: 43.60\n",
      "Epoch [379/500], Step[0/25], Loss: 3.7742, Perplexity: 43.56\n",
      "Epoch [380/500], Step[0/25], Loss: 3.7688, Perplexity: 43.33\n",
      "Epoch [381/500], Step[0/25], Loss: 3.7859, Perplexity: 44.08\n",
      "Epoch [382/500], Step[0/25], Loss: 3.7612, Perplexity: 43.00\n",
      "Epoch [383/500], Step[0/25], Loss: 3.7695, Perplexity: 43.36\n",
      "Epoch [384/500], Step[0/25], Loss: 3.7532, Perplexity: 42.66\n",
      "Epoch [385/500], Step[0/25], Loss: 3.7638, Perplexity: 43.11\n",
      "Epoch [386/500], Step[0/25], Loss: 3.7512, Perplexity: 42.57\n",
      "Epoch [387/500], Step[0/25], Loss: 3.7526, Perplexity: 42.63\n",
      "Epoch [388/500], Step[0/25], Loss: 3.7434, Perplexity: 42.24\n",
      "Epoch [389/500], Step[0/25], Loss: 3.7401, Perplexity: 42.10\n",
      "Epoch [390/500], Step[0/25], Loss: 3.7383, Perplexity: 42.03\n",
      "Epoch [391/500], Step[0/25], Loss: 3.7325, Perplexity: 41.78\n",
      "Epoch [392/500], Step[0/25], Loss: 3.7273, Perplexity: 41.57\n",
      "Epoch [393/500], Step[0/25], Loss: 3.7260, Perplexity: 41.51\n",
      "Epoch [394/500], Step[0/25], Loss: 3.7217, Perplexity: 41.33\n",
      "Epoch [395/500], Step[0/25], Loss: 3.7154, Perplexity: 41.08\n",
      "Epoch [396/500], Step[0/25], Loss: 3.7112, Perplexity: 40.90\n",
      "Epoch [397/500], Step[0/25], Loss: 3.7053, Perplexity: 40.66\n",
      "Epoch [398/500], Step[0/25], Loss: 3.7034, Perplexity: 40.58\n",
      "Epoch [399/500], Step[0/25], Loss: 3.7088, Perplexity: 40.81\n",
      "Epoch [400/500], Step[0/25], Loss: 3.6968, Perplexity: 40.32\n",
      "Epoch [401/500], Step[0/25], Loss: 3.6906, Perplexity: 40.07\n",
      "Epoch [402/500], Step[0/25], Loss: 3.6968, Perplexity: 40.32\n",
      "Epoch [403/500], Step[0/25], Loss: 3.6983, Perplexity: 40.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [404/500], Step[0/25], Loss: 3.6905, Perplexity: 40.06\n",
      "Epoch [405/500], Step[0/25], Loss: 3.6852, Perplexity: 39.85\n",
      "Epoch [406/500], Step[0/25], Loss: 3.6773, Perplexity: 39.54\n",
      "Epoch [407/500], Step[0/25], Loss: 3.6751, Perplexity: 39.45\n",
      "Epoch [408/500], Step[0/25], Loss: 3.6716, Perplexity: 39.32\n",
      "Epoch [409/500], Step[0/25], Loss: 3.6674, Perplexity: 39.15\n",
      "Epoch [410/500], Step[0/25], Loss: 3.6633, Perplexity: 38.99\n",
      "Epoch [411/500], Step[0/25], Loss: 3.6631, Perplexity: 38.98\n",
      "Epoch [412/500], Step[0/25], Loss: 3.6697, Perplexity: 39.24\n",
      "Epoch [413/500], Step[0/25], Loss: 3.6646, Perplexity: 39.04\n",
      "Epoch [414/500], Step[0/25], Loss: 3.6562, Perplexity: 38.71\n",
      "Epoch [415/500], Step[0/25], Loss: 3.6518, Perplexity: 38.55\n",
      "Epoch [416/500], Step[0/25], Loss: 3.6529, Perplexity: 38.59\n",
      "Epoch [417/500], Step[0/25], Loss: 3.6476, Perplexity: 38.38\n",
      "Epoch [418/500], Step[0/25], Loss: 3.6490, Perplexity: 38.44\n",
      "Epoch [419/500], Step[0/25], Loss: 3.6427, Perplexity: 38.20\n",
      "Epoch [420/500], Step[0/25], Loss: 3.6431, Perplexity: 38.21\n",
      "Epoch [421/500], Step[0/25], Loss: 3.6408, Perplexity: 38.12\n",
      "Epoch [422/500], Step[0/25], Loss: 3.6324, Perplexity: 37.81\n",
      "Epoch [423/500], Step[0/25], Loss: 3.6362, Perplexity: 37.95\n",
      "Epoch [424/500], Step[0/25], Loss: 3.6266, Perplexity: 37.59\n",
      "Epoch [425/500], Step[0/25], Loss: 3.6247, Perplexity: 37.51\n",
      "Epoch [426/500], Step[0/25], Loss: 3.6212, Perplexity: 37.38\n",
      "Epoch [427/500], Step[0/25], Loss: 3.6230, Perplexity: 37.45\n",
      "Epoch [428/500], Step[0/25], Loss: 3.6215, Perplexity: 37.39\n",
      "Epoch [429/500], Step[0/25], Loss: 3.6223, Perplexity: 37.42\n",
      "Epoch [430/500], Step[0/25], Loss: 3.6170, Perplexity: 37.22\n",
      "Epoch [431/500], Step[0/25], Loss: 3.6130, Perplexity: 37.08\n",
      "Epoch [432/500], Step[0/25], Loss: 3.6137, Perplexity: 37.10\n",
      "Epoch [433/500], Step[0/25], Loss: 3.6043, Perplexity: 36.75\n",
      "Epoch [434/500], Step[0/25], Loss: 3.6160, Perplexity: 37.19\n",
      "Epoch [435/500], Step[0/25], Loss: 3.6031, Perplexity: 36.71\n",
      "Epoch [436/500], Step[0/25], Loss: 3.6053, Perplexity: 36.79\n",
      "Epoch [437/500], Step[0/25], Loss: 3.5990, Perplexity: 36.56\n",
      "Epoch [438/500], Step[0/25], Loss: 3.5931, Perplexity: 36.35\n",
      "Epoch [439/500], Step[0/25], Loss: 3.6037, Perplexity: 36.74\n",
      "Epoch [440/500], Step[0/25], Loss: 3.5845, Perplexity: 36.04\n",
      "Epoch [441/500], Step[0/25], Loss: 3.5920, Perplexity: 36.30\n",
      "Epoch [442/500], Step[0/25], Loss: 3.5937, Perplexity: 36.37\n",
      "Epoch [443/500], Step[0/25], Loss: 3.5808, Perplexity: 35.90\n",
      "Epoch [444/500], Step[0/25], Loss: 3.5886, Perplexity: 36.18\n",
      "Epoch [445/500], Step[0/25], Loss: 3.5770, Perplexity: 35.77\n",
      "Epoch [446/500], Step[0/25], Loss: 3.5713, Perplexity: 35.56\n",
      "Epoch [447/500], Step[0/25], Loss: 3.5643, Perplexity: 35.31\n",
      "Epoch [448/500], Step[0/25], Loss: 3.5652, Perplexity: 35.35\n",
      "Epoch [449/500], Step[0/25], Loss: 3.5612, Perplexity: 35.20\n",
      "Epoch [450/500], Step[0/25], Loss: 3.5672, Perplexity: 35.42\n",
      "Epoch [451/500], Step[0/25], Loss: 3.5603, Perplexity: 35.17\n",
      "Epoch [452/500], Step[0/25], Loss: 3.5606, Perplexity: 35.18\n",
      "Epoch [453/500], Step[0/25], Loss: 3.5581, Perplexity: 35.10\n",
      "Epoch [454/500], Step[0/25], Loss: 3.5522, Perplexity: 34.89\n",
      "Epoch [455/500], Step[0/25], Loss: 3.5474, Perplexity: 34.72\n",
      "Epoch [456/500], Step[0/25], Loss: 3.5479, Perplexity: 34.74\n",
      "Epoch [457/500], Step[0/25], Loss: 3.5449, Perplexity: 34.64\n",
      "Epoch [458/500], Step[0/25], Loss: 3.5449, Perplexity: 34.64\n",
      "Epoch [459/500], Step[0/25], Loss: 3.5397, Perplexity: 34.46\n",
      "Epoch [460/500], Step[0/25], Loss: 3.5395, Perplexity: 34.45\n",
      "Epoch [461/500], Step[0/25], Loss: 3.5380, Perplexity: 34.40\n",
      "Epoch [462/500], Step[0/25], Loss: 3.5307, Perplexity: 34.15\n",
      "Epoch [463/500], Step[0/25], Loss: 3.5231, Perplexity: 33.89\n",
      "Epoch [464/500], Step[0/25], Loss: 3.5411, Perplexity: 34.50\n",
      "Epoch [465/500], Step[0/25], Loss: 3.5196, Perplexity: 33.77\n",
      "Epoch [466/500], Step[0/25], Loss: 3.5173, Perplexity: 33.69\n",
      "Epoch [467/500], Step[0/25], Loss: 3.5206, Perplexity: 33.81\n",
      "Epoch [468/500], Step[0/25], Loss: 3.5277, Perplexity: 34.04\n",
      "Epoch [469/500], Step[0/25], Loss: 3.5119, Perplexity: 33.51\n",
      "Epoch [470/500], Step[0/25], Loss: 3.5198, Perplexity: 33.78\n",
      "Epoch [471/500], Step[0/25], Loss: 3.5091, Perplexity: 33.42\n",
      "Epoch [472/500], Step[0/25], Loss: 3.5042, Perplexity: 33.25\n",
      "Epoch [473/500], Step[0/25], Loss: 3.5058, Perplexity: 33.31\n",
      "Epoch [474/500], Step[0/25], Loss: 3.5059, Perplexity: 33.31\n",
      "Epoch [475/500], Step[0/25], Loss: 3.4991, Perplexity: 33.09\n",
      "Epoch [476/500], Step[0/25], Loss: 3.4917, Perplexity: 32.84\n",
      "Epoch [477/500], Step[0/25], Loss: 3.4942, Perplexity: 32.92\n",
      "Epoch [478/500], Step[0/25], Loss: 3.4883, Perplexity: 32.73\n",
      "Epoch [479/500], Step[0/25], Loss: 3.4829, Perplexity: 32.56\n",
      "Epoch [480/500], Step[0/25], Loss: 3.5007, Perplexity: 33.14\n",
      "Epoch [481/500], Step[0/25], Loss: 3.4865, Perplexity: 32.67\n",
      "Epoch [482/500], Step[0/25], Loss: 3.4768, Perplexity: 32.36\n",
      "Epoch [483/500], Step[0/25], Loss: 3.4866, Perplexity: 32.68\n",
      "Epoch [484/500], Step[0/25], Loss: 3.4734, Perplexity: 32.25\n",
      "Epoch [485/500], Step[0/25], Loss: 3.4825, Perplexity: 32.54\n",
      "Epoch [486/500], Step[0/25], Loss: 3.4726, Perplexity: 32.22\n",
      "Epoch [487/500], Step[0/25], Loss: 3.4741, Perplexity: 32.27\n",
      "Epoch [488/500], Step[0/25], Loss: 3.4821, Perplexity: 32.53\n",
      "Epoch [489/500], Step[0/25], Loss: 3.4673, Perplexity: 32.05\n",
      "Epoch [490/500], Step[0/25], Loss: 3.4619, Perplexity: 31.88\n",
      "Epoch [491/500], Step[0/25], Loss: 3.4645, Perplexity: 31.96\n",
      "Epoch [492/500], Step[0/25], Loss: 3.4667, Perplexity: 32.03\n",
      "Epoch [493/500], Step[0/25], Loss: 3.4656, Perplexity: 32.00\n",
      "Epoch [494/500], Step[0/25], Loss: 3.4677, Perplexity: 32.06\n",
      "Epoch [495/500], Step[0/25], Loss: 3.4598, Perplexity: 31.81\n",
      "Epoch [496/500], Step[0/25], Loss: 3.4534, Perplexity: 31.61\n",
      "Epoch [497/500], Step[0/25], Loss: 3.4601, Perplexity: 31.82\n",
      "Epoch [498/500], Step[0/25], Loss: 3.4577, Perplexity: 31.74\n",
      "Epoch [499/500], Step[0/25], Loss: 3.4484, Perplexity: 31.45\n",
      "Epoch [500/500], Step[0/25], Loss: 3.4490, Perplexity: 31.47\n",
      "CPU times: user 26min 6s, sys: 10min 25s, total: 36min 31s\n",
      "Wall time: 34min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "loss_ot = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        loss_ot.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (embed): Embedding(4389, 128)\n",
       "  (lstm): LSTM(128, 2048, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=2048, out_features=4389, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"Loss for \" + title)\n",
    "plt.plot(loss_ot)\n",
    "plt.savefig(\"imgs/\" + title, dpi=420)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metrics/\" + title + \".txt\", 'w') as f:\n",
    "    f.write(\"Loss: \" + str(loss_ot[-1]) + \"\\n\")\n",
    "    f.write(\"Perplexity: \" + str(np.exp(loss_ot[-1])) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [200/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [300/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [400/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [500/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [600/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [700/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [800/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [900/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n",
      "Sampled [1000/1000] words and save to outputs/LSTM-L-2-H-2048-Dropout-No-tanh-SGD.txt\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    with open(\"outputs/\" + title + '.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly\n",
    "        prob = torch.ones(vocab_size)\n",
    "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Forward propagate RNN \n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Fill input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # File write\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, \"outputs/\" + title + '.txt'))\n",
    "\n",
    "f.close()\n",
    "# Save the model checkpoints\n",
    "# torch.save(model.state_dict(), 'saved_models/' + title + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
